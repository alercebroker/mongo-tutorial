{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd3e27c-921d-49a5-995e-0d9535177bb5",
   "metadata": {},
   "source": [
    "## Aggregation pipelines\n",
    "\n",
    "In MongoDB it is possible to concatenate multiple operations within a single command by using aggregation pipelines. This can include some of the things we've seen before, such as queries, projections, sorting and pagination, and also additional operations which are not available as part of the previous commands.\n",
    "\n",
    "An aggregation pipeline in MongoDB consists of one or more stage, each with their own possible operators. Most stages are rather free when it comes to the order in which they have to be used or how many times they can be used, but keep in mind that some of them have restrictions about when and how many times they can be used.\n",
    "\n",
    "To use an aggregation pipeline in pymongo, we use the `aggregate` method for collections. This receives a list of dictionaries as its main parameter. Each dictionary must have a single key, corresponding to the pipeline stage and the value defining the working of the stage. The output of each stage will then be used as input for the following, all the way until the list is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602f927-3b00-47ac-9d8e-ed538eea303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(host='localhost', port=27017, username='mongo', password='mongo')\n",
    "\n",
    "objects = client.alerce.objects  # This is the collection we'll be using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b16a2-7a1d-4e0b-accf-d21f4d4ecab4",
   "metadata": {},
   "source": [
    "We'll now check some of the more commonly used stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ee6b1-2498-456a-a54f-1f2eab4e8664",
   "metadata": {},
   "source": [
    "### `$match`\n",
    "\n",
    "The stage `$match` is equivalent to performing a query and uses the same operators we've seen for the `find` method first parameter. This stage doesn't include the possibility of using a projection (there is a special stage for that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a83f9-9766-4f0d-8ae0-444356e66d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([{'$match': {'ndet': {'$gte': 400}}}])\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17834aff-05d5-4e79-a967-b2923c63ca2a",
   "metadata": {},
   "source": [
    "The output of `aggregate` is a `CommandCursor`. This is different from the `Cursor` we saw for the output of `find`, but it is still iterable. \n",
    "\n",
    "Unfortunately, this type does not have the `explain` method implemented, although there is a somewhat cumbersome workaround if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42bf8a-98cf-446e-97ec-60afde5b843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.alerce  # We need the database, not the collection\n",
    "           # method     # collec   # argument (by name as keyword argument)        # explain\n",
    "db.command('aggregate', 'objects', pipeline=[{'$match': {'ndet': {'$gte': 400}}}], explain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48bb00-74c2-43f4-abe8-8cdc99e6ff8b",
   "metadata": {},
   "source": [
    "**`mongosh`:** The aggregate method works in the exact same way, although in this case the `explain` method is actually implemented:\n",
    "\n",
    "```bash\n",
    "   db.objects.aggregate([{$match: {ndet: {$gte: 400}}}]).explain()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bd4a56-7a53-4e28-b348-c0bacac81db4",
   "metadata": {},
   "source": [
    "### `$project`\n",
    "\n",
    "As the name implies, the `$project` stage is equivalent to the projection we've seen in the previous module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0cf774-f93b-4c54-8756-c6a5f3a5aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([{'$project': {'ndet': True, '_id': False}}])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf247a6-f714-4c67-a613-eac32b5031bb",
   "metadata": {},
   "source": [
    "Each stage can be concatenated in any order. Keep in mind that the field being used might change due to renaming and the order of the stages. The following two blocks give the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe0dfa-3453-425c-850b-45052b71dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {'$project': {'detections': '$ndet', '_id': False}},  # Renaming the field\n",
    "    {'$match': {'detections': {'$gte': 400}}}  # We need to use the new name\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09bdc47-0a68-43ce-ae4d-6ca03d82d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {'$match': {'ndet': {'$gte': 400}}},  # Using ndet    \n",
    "    {'$project': {'detections': '$ndet', '_id': False}},  # Renaming the field\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1efc45-7846-484d-9882-c867650355bb",
   "metadata": {},
   "source": [
    "However, it is important to note that, in terms of performance they are both very different. By starting with the match, we only need to rename the field for the 3 documents matched documents. Using the reverse order, we'll be renaming the field for the whole collection and then selecting the relevant documents.\n",
    "\n",
    "**It is recommended to start a pipeline with a `$match` that limits as much as possible the number of results since this is the only stage that uses indexing. Other stages applied over large collections or poorly filtered results can take a long time to finish.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38688c46-aac7-40b3-9177-8403142eb465",
   "metadata": {},
   "source": [
    "### `$set` and `$addFields`\n",
    "\n",
    "These stages do the same thing, although `$set` is only available starting on MongoDB 4.2. Their behaviour is similar to that of `$project` when creating a new field. The difference comes in the fact that the new fields are added to the existing ones instead of having to select what is going to be in the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff71bce-682c-44a6-b75d-ffd2b76b1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {'$match': {'ndet': {'$gte': 400}}},\n",
    "    {'$set': {'deltamjd': {'$subtract': ['$lastmjd', '$firstmjd']}}},\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b43139f-0404-4d1e-96b2-f930ac63c81a",
   "metadata": {},
   "source": [
    "The new fields are always added at the end of the dictionary. More than one field can be added in a single stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439e77b-293b-4ee2-b3e2-f1b1d3958fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {'$match': {'ndet': {'$gte': 400}}},\n",
    "    {'$set': {\n",
    "        'deltamjd': {\n",
    "            '$subtract': ['$lastmjd', '$firstmjd']\n",
    "        },\n",
    "        'stamp_classified': {  # Checks if at least one of the classifier names contains 'stamp_classifier'\n",
    "            '$in': ['stamp_classifier', '$probabilities.classifier_name']\n",
    "        }\n",
    "    }},\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba4584-100f-4353-83de-5ac31591d8cb",
   "metadata": {},
   "source": [
    "### `$unwind`\n",
    "\n",
    "The stage `$unwind` is used for arrays and it will \"disassemble\" the array, resulting on one document for each array element among all the retrieved documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9cc0d-3279-4271-aa64-08881660c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {'$match': {'ndet': {'$gte': 400}}},\n",
    "    {'$unwind': '$probabilities'},\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b0f70-7917-4d63-8cd4-13d12234ea9b",
   "metadata": {},
   "source": [
    "As you can see, now we retrieved repeated `_id`s, and each output document correspond to one element of the original `probabilities` array. The field `probabilities` is now an element of the array we began with.\n",
    "\n",
    "This allows us to get a single entry when searching by, for instance, class and probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18001080-41a7-44da-92d9-86986ff85c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = 'stamp_classifier'\n",
    "class_ = 'VS'\n",
    "min_prob = 0.7\n",
    "\n",
    "docs = objects.aggregate([\n",
    "    {\n",
    "        '$match': {\n",
    "            'probabilities': {\n",
    "                '$elemMatch': {\n",
    "                    'classifier_name': classifier,\n",
    "                    'class_name': class_,\n",
    "                    'probability': {'$gte': min_prob}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {  # Remember that in the last stage we still have the full 'probabilities' array\n",
    "        '$set': {\n",
    "            'probabilities': {\n",
    "                '$filter': {\n",
    "                    'input': '$probabilities',\n",
    "                    'cond': {\n",
    "                        '$and': [\n",
    "                            {'$eq': ['$$this.classifier_name', classifier]},\n",
    "                            {'$eq': ['$$this.class_name', class_]},\n",
    "                            {'$gte': ['$$this.probability', min_prob]}\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$unwind': '$probabilities'\n",
    "    },\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376e0e1-7da3-4121-bb67-03cd455b8661",
   "metadata": {},
   "source": [
    "**Warning:** Unfortunately, due to how the selection of array elements works, both the `$set` and the `$match` stages should match for a query like the one above, but there is no control over it. It is very easy to be testing some queries and then forget to update the value or the field in either the `$filter` or the `$elemMatch` operators, resulting in valid but meaningless queries. *Pay close attention when creating this types of queries.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a94a61-6924-4ec4-b1fc-8bb451c5e1b8",
   "metadata": {},
   "source": [
    "### `$group`\n",
    "\n",
    "This stage allows grouping the results based on an expression. For instance, we'll search here for the first ranked class and then group the results based on the class name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dbcd36-81f7-4b9a-a695-ec4bbfc887bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = 'stamp_classifier'\n",
    "ranking = 1\n",
    "\n",
    "docs = objects.aggregate([\n",
    "    {\n",
    "        '$match': {\n",
    "            'probabilities': {\n",
    "                '$elemMatch': {\n",
    "                    'classifier_name': classifier,\n",
    "                    'ranking': ranking\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$set': {\n",
    "            'probabilities': {\n",
    "                '$filter': {\n",
    "                    'input': '$probabilities',\n",
    "                    'cond': {\n",
    "                        '$and': [\n",
    "                            {'$eq': ['$$this.classifier_name', classifier]},\n",
    "                            {'$eq': ['$$this.ranking', ranking]}\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$unwind': '$probabilities'\n",
    "    },\n",
    "    {\n",
    "        '$group': {\n",
    "            '_id': '$probabilities.class_name'\n",
    "        }\n",
    "    }\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56321f3c-3e52-42c9-80a7-96969304434d",
   "metadata": {},
   "source": [
    "Above can be seen that, by default, only the `_id` field is present. This is always the field or expression on which the grouping will be based. Any other field on the output can be defined in the same `$group` stage and also allow for expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3347703-fbc4-4f3b-b2c6-11916431dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = 'stamp_classifier'\n",
    "ranking = 1\n",
    "\n",
    "docs = objects.aggregate([\n",
    "    {\n",
    "        '$match': {\n",
    "            'probabilities': {\n",
    "                '$elemMatch': {\n",
    "                    'classifier_name': classifier,\n",
    "                    'ranking': ranking\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$set': {\n",
    "            'probabilities': {\n",
    "                '$filter': {\n",
    "                    'input': '$probabilities',\n",
    "                    'cond': {\n",
    "                        '$and': [\n",
    "                            {'$eq': ['$$this.classifier_name', classifier]},\n",
    "                            {'$eq': ['$$this.ranking', ranking]}\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$unwind': '$probabilities'\n",
    "    },\n",
    "    {\n",
    "        '$group': {\n",
    "            '_id': '$probabilities.class_name',\n",
    "            'nobjects': {'$count': {}},  # Count documents (always receives empty dict)\n",
    "            'earliestmjd': {'$min': '$firstmjd'},\n",
    "            'latestmjd': {'$max': '$lastmjd'}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$match': {\n",
    "            'nobjects': {'$gte': 10}  # keep only groups with more than ten elements\n",
    "        }\n",
    "    }\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3070e8-8c5e-4703-9563-b373e942a624",
   "metadata": {},
   "source": [
    "Note that `$count` is only available from MongoDB version 5.0 onwards. For older versions, an analogous behaviour can be achieved using `{'$sum': 1}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a6d92-e23f-4812-bd95-303b85953a4f",
   "metadata": {},
   "source": [
    "### `$bucket` and `$bucketAuto`\n",
    "\n",
    "These stages are similar to `$group`, but now the grouping is based on binning rather than the results of an expression. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1fabe-a6bc-476e-81e1-603e0f3d05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {\n",
    "        '$bucketAuto': {\n",
    "            'groupBy': '$ndet',  # receives an expression\n",
    "            'buckets': 4,  # number of buckets\n",
    "            'output': {  # fields in the output document\n",
    "                'count': {'$count': {}},\n",
    "                'minmjd': {'$min': '$firstmjd'},\n",
    "                'maxmjd': {'$max': '$lastmjd'}\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abffa47-bcd2-49ed-be6e-7e8a392d1fd8",
   "metadata": {},
   "source": [
    "As can be seen above, `$bucketAuto` will automatically generate the bins, trying to split the documents as evenly as possible. The limits are stored in the field `_id` of the returned documents. There is the additional option for granularity to try and fix the limits to specified levels (in the example below, using the [Renard series](https://en.wikipedia.org/wiki/Renard_series) R10; for other options check the [documentation](https://www.mongodb.com/docs/v6.0/reference/operator/aggregation/bucketAuto/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d48b7a-d9f5-4db3-847a-f51c8adabb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {\n",
    "        '$bucketAuto': {\n",
    "            'groupBy': '$ndet',  # receives an expression\n",
    "            'buckets': 4,  # number of buckets\n",
    "            'output': {  # fields in the output document\n",
    "                'count': {'$count': {}},\n",
    "                'minmjd': {'$min': '$firstmjd'},\n",
    "                'maxmjd': {'$max': '$lastmjd'}\n",
    "            },\n",
    "            'granularity': 'R10'  # Using Renard series\n",
    "        }\n",
    "    },\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07acf2f0-6e03-4763-b989-77ccd86fcc62",
   "metadata": {},
   "source": [
    "In the case of `$bucket`, the binning limits must be explicitly provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f2119-ac5f-4004-a368-14ed373c6496",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = 'stamp_classifier'\n",
    "\n",
    "docs = objects.aggregate([\n",
    "    {\n",
    "        '$match': {\n",
    "            'probabilities.classifier_name': classifier,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$set': {\n",
    "            'probabilities': {\n",
    "                '$filter': {\n",
    "                    'input': '$probabilities',\n",
    "                    'cond': {\n",
    "                        '$eq': ['$$this.classifier_name', classifier],\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$unwind': '$probabilities'\n",
    "    },\n",
    "    {\n",
    "        '$bucket': {\n",
    "            'groupBy': '$probabilities.probability',\n",
    "            'boundaries': [0, .25, .5, .75, 1],  # bin boundaries\n",
    "            'output': {\n",
    "                'nobjects': {'$count': {}},\n",
    "                'top_class': {\n",
    "                    '$top': {  # get top (first) result based on a sort\n",
    "                        'output': '$probabilities.class_name',  # can be any expression\n",
    "                        'sortBy': {'probabilities.probability': -1}\n",
    "                    }\n",
    "                },\n",
    "                'ids': {'$addToSet': '$_id'}  # generate array with _id of all documents in bucket\n",
    "            },\n",
    "            'default': 'oob'  # documents outside the buckets will be put in this _id\n",
    "        }\n",
    "    }\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7e10d-1317-462d-852f-047fce7312c9",
   "metadata": {},
   "source": [
    "The bin boundaries are always inclusive of the bottom limit and exclusive of the top. This explains the out of bound result we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57f890-1d3d-494d-b575-91027deef858",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects.find_one({'_id': 'AL17ldggwgicoxfuy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13038d78-08db-409c-b046-bb9c86c8cbc1",
   "metadata": {},
   "source": [
    "The classification in version 1.0.4 of the stamp classifier gives an exact 1 for the probabiltity of this object being an asteroid, thus falling outside the boundary of our bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d35e99-2e02-4e4c-ad1d-c78b8df9c595",
   "metadata": {},
   "source": [
    "### `$sort`, `$limit` and `$skip`\n",
    "\n",
    "The `aggregate` method doesn't support sort, limit and skip as options in the way we saw for `find`. They can still be used, but as stages in the pipeline. Both `$skip` and `$limit` are used in the same way as the option based usage we saw for `find`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7261249-393f-4602-9b0a-590c9e9b48d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {'$project': {'ndet': True, 'firstmjd': True, 'lastmjd': True}},\n",
    "    {'$skip': 5},\n",
    "    {'$limit': 10}\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52757bc-797c-4136-b847-6b4c8a031a1d",
   "metadata": {},
   "source": [
    "Note that the order matters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c442956-e94f-454d-9463-e5eb98b389e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {'$project': {'ndet': True, 'firstmjd': True, 'lastmjd': True}},\n",
    "    {'$limit': 10},\n",
    "    {'$skip': 5},\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5731490c-6e50-4043-86d5-36f5d16e5031",
   "metadata": {},
   "source": [
    "In the first usage we skipped the first five results and then return up to ten documents. In the second we return up to ten documents (without skipping), and *then* we skip the first five of those documents.\n",
    "\n",
    "For `$sort`, we now use a dictionary that relates the field and the direction, unlike the use as an option for `find`, where we had to use a list of pairs. Otherwise, the results are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8e5e4-0911-404e-922d-558dae437874",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {'$project': {'ndet': True, 'firstmjd': True, 'lastmjd': True}},\n",
    "    {'$sort': {'ndet': 1, 'firstmjd': -1}},  # sort first by ndet ascending, then by firstmjd descending\n",
    "    {'$skip': 5},\n",
    "    {'$limit': 10}\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c225e50-dddc-48ab-b242-7b3fba966fe9",
   "metadata": {},
   "source": [
    "### `$replaceRoot` and `$replaceWith`\n",
    "\n",
    "These stages work in the same way, but have slightly different syntax, with `$replaceWith` only avalable starting in MongoDB 4.2. They work by accessing nested documents and returning those as a result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cad945-ecce-4466-93b7-dd4804f32f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {'$unwind': '$probabilities'},\n",
    "    {'$replaceWith': '$probabilities'},  # use the probabilities subdocument as return documents\n",
    "    {'$limit': 5}\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92050f85-dd2b-4d9f-b4fe-26cb580bdaed",
   "metadata": {},
   "source": [
    "In the case of `$replaceRoot`, the option needs to be passed somewhat differently, but otherwise works in the exact same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb379b-df4d-4bba-aea1-425af9dc1c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {'$unwind': '$probabilities'},\n",
    "    {'$replaceRoot': {'newRoot': '$probabilities'}},\n",
    "    {'$limit': 5}\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01327255-76d2-4a58-b3a0-4428c4ef3630",
   "metadata": {},
   "source": [
    "### `$out`\n",
    "\n",
    "This stage is only available starting on MongoDB 4.4. Unlike other stages, this *must* be the last stage and thus cannot be in the pipeline more than once. It is used to create a new collection with the results or write the results into an existing collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d1cbe-af9d-497b-9c9c-82a218da65e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {'$unwind': '$probabilities'},\n",
    "    {'$replaceWith': {'$mergeObjects': [{'aid': '$_id'}, '$probabilities']}},  # add the aid to the probabilities document \n",
    "    {'$out': {'db': 'alerce', 'coll': 'probabilities'}}  # write in (existing) database 'alerce' and (new) collection 'probabilities'\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6cb06-6c34-4cc7-97c1-36fd39aef924",
   "metadata": {},
   "source": [
    "You can notice that there is no output when circulating `docs`. This is normal behaviour when using `$out`. Let's check the new collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13767b-a233-434e-af53-1ec0697cfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = client.alerce.probabilities.find({}, limit=4)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5f360-afd4-4a63-bcf8-8df840fcfa23",
   "metadata": {},
   "source": [
    "### `$lookup`\n",
    "\n",
    "This stage allows for joining two collections (they must be in the same database) based on a shared key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aea85a-3724-43cc-b2c9-6cef52f4afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "docs = objects.aggregate([\n",
    "    {'$project': {'probabilities': False}},  # remove existing probabilities to avoid duplicate field\n",
    "    {\n",
    "        '$lookup': {\n",
    "            'from': 'probabilities',  # collection to join\n",
    "            'localField': '_id',  # field in current collection (objects) to perform the join over\n",
    "            'foreignField': 'aid',  # field in external collection (probabilities) to perform the join over\n",
    "            'as': 'probs'  # name of the joint field in the collection\n",
    "        }\n",
    "    },\n",
    "    {'$limit': 1}  #  we'll see one as an example\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5672006d-9e02-4e4d-8120-4410b7f47f22",
   "metadata": {},
   "source": [
    "It is also possible to use another pipeline within the second collection (starting on MongoDB 5.0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714bf12-f9d6-4773-8afa-5f2d597387f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {\n",
    "        '$match': {\n",
    "            'ndet': {'$gt': 200, '$lt': 400}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project': {\n",
    "            'probabilities': False  # remove existing probabilities to avoid duplicate field\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$lookup': {\n",
    "            'from': 'probabilities',  # collection to join\n",
    "            'localField': '_id',  # field in current collection (objects) to perform the join over\n",
    "            'foreignField': 'aid',  # field in external collection (probabilities) to perform the join over\n",
    "            'as': 'probs',  # name of the joint field in the collection\n",
    "            'pipeline': [\n",
    "                {\n",
    "                    '$match': {\n",
    "                        'classifier_name': 'stamp_classifier', \n",
    "                        'ranking': 1,\n",
    "                        'class_name': 'VS',\n",
    "                        'probability': {'$gte': 0.7}\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    '$project': {\n",
    "                        '_id': False,\n",
    "                        'aid': False\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf95e8d-4325-4286-8e77-2c09da9930e5",
   "metadata": {},
   "source": [
    "You can notice that most objects have empty probabilities. That is because all objects from the collection on which `aggregate` is called that are present at the stage of the pipeline before `$lookup` will always be preserved, even if there is no match in the collection we're joining from. Essentially, it is always a left outer join. We can use another stage to mimic an inner join, but keep in mind that if the input collection is too large, this step might be highly inefficient, even in the second collection has very few elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5c8e1-8686-43ae-8070-550d7111e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = objects.aggregate([\n",
    "    {\n",
    "        '$match': {\n",
    "            'ndet': {'$gt': 200, '$lt': 400}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project': {\n",
    "            'probabilities': False  # remove existing probabilities to avoid duplicate field\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$lookup': {\n",
    "            'from': 'probabilities',  # collection to join\n",
    "            'localField': '_id',  # field in current collection (objects) to perform the join over\n",
    "            'foreignField': 'aid',  # field in external collection (probabilities) to perform the join over\n",
    "            'as': 'probs',  # name of the joint field in the collection\n",
    "            'pipeline': [\n",
    "                {\n",
    "                    '$match': {\n",
    "                        'classifier_name': 'stamp_classifier', \n",
    "                        'ranking': 1,\n",
    "                        'class_name': 'VS',\n",
    "                        'probability': {'$gte': 0.7}\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    '$project': {\n",
    "                        '_id': False,\n",
    "                        'aid': False\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$match': {\n",
    "            'probs': {'$ne': []}  # only probs that are non-empty lists\n",
    "        }\n",
    "    }\n",
    "])\n",
    "\n",
    "for doc in docs:\n",
    "    pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84385e9e-94e8-4d31-b806-259811af3e5e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "There are several other stages we've not covered here. Check [here](https://www.mongodb.com/docs/v6.0/reference/operator/aggregation-pipeline/#std-label-aggregation-pipeline-operator-reference) for references on all of them, including those seen above.\n",
    "\n",
    "Aggregation pipelines can be used to do more complex queries and have a fine tuned manipulation of the returned fields. However, if a simple query is all that's required, it is best to simply use `find`.\n",
    "\n",
    "Recommendations for performance:\n",
    "\n",
    "* The first stages in the pipeline should limit as much as possible the number of documents\n",
    "* Be mindful of the number of stages, keep the pipeline as short as possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
